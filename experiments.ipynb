{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b5dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "stablemax\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 5000: Training loss: 0.0007\n",
      "Time taken for the last 5000 epochs: 0.36 min\n",
      "Epoch 10000: Training loss: 0.0001\n",
      "Time taken for the last 5000 epochs: 0.33 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.34 min\n",
      "Epoch 20000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.33 min\n",
      "Epoch 25000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 30000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 35000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.26 min\n",
      "Epoch 40000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.28 min\n",
      "Epoch 45000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 50000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 55000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 60000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.28 min\n",
      "Epoch 65000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 70000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.27 min\n",
      "Epoch 75000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.26 min\n",
      "Test set: Average loss: 909345.2734, Accuracy: 98.63\n",
      "Saving run: add_mod|num_epochs-80000|train_fraction-0.4|loss_function-stablemax|log_frequency-5000|lr-0.01|batch_size-5107|beta2-0.999\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 10: Training loss: 4.5250\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 20: Training loss: 4.1646\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 30: Training loss: 3.8555\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 40: Training loss: 3.6096\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 50: Training loss: 3.4165\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 60: Training loss: 3.2433\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 70: Training loss: 3.0426\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 80: Training loss: 2.7112\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 90: Training loss: 2.1663\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 100: Training loss: 1.4031\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 110: Training loss: 0.6555\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 120: Training loss: 0.2968\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 130: Training loss: 0.1674\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 140: Training loss: 0.1153\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 150: Training loss: 0.0885\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 160: Training loss: 0.0712\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 170: Training loss: 0.0591\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 180: Training loss: 0.0513\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 190: Training loss: 0.0462\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 200: Training loss: 0.0427\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 210: Training loss: 0.0402\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 220: Training loss: 0.0383\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 230: Training loss: 0.0367\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 240: Training loss: 0.0353\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 250: Training loss: 0.0341\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 260: Training loss: 0.0330\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 270: Training loss: 0.0320\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 280: Training loss: 0.0311\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Epoch 290: Training loss: 0.0302\n",
      "Time taken for the last 10 epochs: 0.00 min\n",
      "Test set: Average loss: 0.0553, Accuracy: 100.00\n",
      "Saving run: add_mod|num_epochs-300|train_fraction-0.4|log_frequency-10|lr-0.01|batch_size-5107|orthogonal_gradients-True\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.23 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.21 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.21 min\n",
      "Epoch 20000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 25000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 30000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.21 min\n",
      "Epoch 35000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.20 min\n",
      "Epoch 40000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.13 min\n",
      "Epoch 45000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.11 min\n",
      "Epoch 50000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.11 min\n",
      "Epoch 55000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.12 min\n",
      "Epoch 60000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.13 min\n",
      "Epoch 65000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.13 min\n",
      "Epoch 70000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.13 min\n",
      "Epoch 75000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.12 min\n",
      "Test set: Average loss: 203.8640, Accuracy: 0.72\n",
      "Saving run: add_mod|num_epochs-80000|train_fraction-0.4|log_frequency-5000|lr-0.01|batch_size-5107|softmax_precision-64\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7266\n",
      "Epoch 500: Training loss: 0.2101\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0035\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0026\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0081\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0088\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0053\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0034\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0017\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0013\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0017\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0025\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0010\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0010\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.2729\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0008\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0007\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0006\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0006\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0018\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0004\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0003\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0021\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0006\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 17000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 59.8356, Accuracy: 0.04\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.4|log_frequency-500|lr-0.0005|batch_size-5107|softmax_precision-16|adam_epsilon-1e-30\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 500: Training loss: 0.2086\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0035\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0550\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 74.4896, Accuracy: 0.04\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.4|log_frequency-500|lr-0.0005|batch_size-5107|adam_epsilon-1e-30\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 500: Training loss: 0.2084\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 1000: Training loss: 0.0035\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 1500: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.04 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 112.3625, Accuracy: 0.12\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.4|log_frequency-500|lr-0.0005|batch_size-5107|softmax_precision-64|adam_epsilon-1e-30\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 7661, Test size: 5108\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7266\n",
      "Epoch 500: Training loss: 0.3425\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 1000: Training loss: 0.0036\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 1500: Training loss: 0.0043\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 2000: Training loss: 0.0003\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0002\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 3000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 4000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.03 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 11.3504, Accuracy: 15.11\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.6|log_frequency-500|lr-0.0005|batch_size-7661|softmax_precision-16\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 7661, Test size: 5108\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 500: Training loss: 0.3265\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0033\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 10.3207, Accuracy: 29.52\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.6|log_frequency-500|lr-0.0005|batch_size-7661\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 7661, Test size: 5108\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 500: Training loss: 0.3326\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0033\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 8.1210, Accuracy: 53.05\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.6|log_frequency-500|lr-0.0005|batch_size-7661|softmax_precision-64\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 8938, Test size: 3831\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7266\n",
      "Epoch 500: Training loss: 0.2242\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0027\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0008\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0050\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 2.9487, Accuracy: 50.95\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.7|log_frequency-500|lr-0.0005|batch_size-8938|softmax_precision-16\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 8938, Test size: 3831\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7274\n",
      "Epoch 500: Training loss: 0.2238\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0023\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0018\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 2.1215, Accuracy: 70.06\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.7|log_frequency-500|lr-0.0005|batch_size-8938\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 8938, Test size: 3831\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7274\n",
      "Epoch 500: Training loss: 0.2223\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1000: Training loss: 0.0024\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 1500: Training loss: 0.0001\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 2500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 3500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 4500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 5500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 6500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 7000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 7500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 8500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 9500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 10500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 11500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 12000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 12500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 13000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.02 min\n",
      "Epoch 13500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 14500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 15500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 16500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 17500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 18500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19000: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Epoch 19500: Training loss: 0.0000\n",
      "Time taken for the last 500 epochs: 0.01 min\n",
      "Test set: Average loss: 0.7850, Accuracy: 90.71\n",
      "Saving run: add_mod|num_epochs-20000|train_fraction-0.7|log_frequency-500|lr-0.0005|batch_size-8938|softmax_precision-64\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "stablemax\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 5000: Training loss: 0.0007\n",
      "Time taken for the last 5000 epochs: 0.17 min\n",
      "Epoch 10000: Training loss: 0.0001\n",
      "Time taken for the last 5000 epochs: 0.16 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.17 min\n",
      "Epoch 20000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.17 min\n",
      "Epoch 25000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.17 min\n",
      "Epoch 30000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.16 min\n",
      "Epoch 35000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 40000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 45000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 50000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 55000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 60000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 65000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 70000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 75000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 80000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.34 min\n",
      "Epoch 85000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.20 min\n",
      "Epoch 90000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.17 min\n",
      "Epoch 95000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.23 min\n",
      "Test set: Average loss: 574318.5938, Accuracy: 99.50\n",
      "Saving run: add_mod|num_epochs-100000|train_fraction-0.4|loss_function-stablemax|log_frequency-5000|lr-0.01|batch_size-5107|beta2-0.999\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "stablemax\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7276\n",
      "Epoch 5000: Training loss: 0.0007\n",
      "Time taken for the last 5000 epochs: 0.19 min\n",
      "Epoch 10000: Training loss: 0.0001\n",
      "Time taken for the last 5000 epochs: 0.21 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 20000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.23 min\n",
      "Epoch 25000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.24 min\n",
      "Epoch 30000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.20 min\n",
      "Epoch 35000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.19 min\n",
      "Epoch 40000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.24 min\n",
      "Epoch 45000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.26 min\n",
      "Epoch 50000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.20 min\n",
      "Epoch 55000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.15 min\n",
      "Epoch 60000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.15 min\n",
      "Epoch 65000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 70000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 75000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.21 min\n",
      "Epoch 80000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.16 min\n",
      "Epoch 85000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.19 min\n",
      "Epoch 90000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.20 min\n",
      "Epoch 95000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.16 min\n",
      "Test set: Average loss: 1014125.5664, Accuracy: 99.00\n",
      "Saving run: add_mod|num_epochs-100000|train_fraction-0.4|loss_function-stablemax|log_frequency-5000|binary_operation-product_mod|lr-0.01|batch_size-5107|beta2-0.999\n",
      "Using device: cuda:0\n",
      "3 40\n",
      "Starting trining. Train dataset size: 1000, Test size: 1000\n",
      "stablemax\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 0.6955\n",
      "Epoch 5000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 10000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.15 min\n",
      "Epoch 15000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.16 min\n",
      "Epoch 20000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.18 min\n",
      "Epoch 25000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 30000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.22 min\n",
      "Epoch 35000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.23 min\n",
      "Epoch 40000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.40 min\n",
      "Epoch 45000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.33 min\n",
      "Epoch 50000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.37 min\n",
      "Epoch 55000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.35 min\n",
      "Epoch 60000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 65000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 70000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 75000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 80000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 85000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 90000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Epoch 95000: Training loss: 0.0000\n",
      "Time taken for the last 5000 epochs: 0.30 min\n",
      "Test set: Average loss: 111387.9844, Accuracy: 99.20\n",
      "Saving run: sparse_parity|num_epochs-100000|train_fraction-0.5|loss_function-stablemax|log_frequency-5000|lr-0.01|batch_size-1000|dataset-sparse_parity|num_noise_features-40|num_parity_features-3|num_samples-2000|beta2-0.999|adam_epsilon-1e-18\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5017, Test size: 7527\n",
      "Using AlgorithmicDataset\n",
      "/home/rohit/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.8941\n",
      "Epoch 200: Training loss: 1.4474\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 400: Training loss: 0.3940\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 600: Training loss: 0.1694\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 800: Training loss: 0.1100\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1000: Training loss: 0.0807\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1200: Training loss: 0.0605\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1400: Training loss: 0.0462\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1600: Training loss: 0.0368\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1800: Training loss: 0.0291\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2000: Training loss: 0.0238\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2200: Training loss: 0.0209\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2400: Training loss: 0.0171\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2600: Training loss: 0.0178\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2800: Training loss: 0.0148\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3000: Training loss: 0.0155\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3200: Training loss: 0.0148\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3400: Training loss: 0.0123\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3600: Training loss: 0.0135\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3800: Training loss: 0.0117\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4000: Training loss: 0.0111\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4200: Training loss: 0.0104\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4400: Training loss: 0.0116\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4600: Training loss: 0.0115\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4800: Training loss: 0.0103\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Test set: Average loss: 0.0017, Accuracy: 100.00\n",
      "Saving run: add_mod|num_epochs-5000|train_fraction-0.4|log_frequency-200|lr-0.001|batch_size-5017|orthogonal_gradients-True|use_transformer-True\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5017, Test size: 7527\n",
      "Using AlgorithmicDataset\n",
      "/home/rohit/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.8941\n",
      "Epoch 200: Training loss: 0.3275\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 400: Training loss: 0.1419\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 600: Training loss: 0.0787\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 800: Training loss: 0.0427\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1000: Training loss: 0.0247\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1200: Training loss: 0.0173\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1400: Training loss: 0.0117\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1600: Training loss: 0.0089\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1800: Training loss: 0.0078\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2000: Training loss: 0.0073\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2200: Training loss: 0.0086\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2400: Training loss: 0.0052\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2600: Training loss: 0.0062\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2800: Training loss: 0.0050\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3000: Training loss: 0.0062\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3200: Training loss: 0.0065\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3400: Training loss: 0.0052\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3600: Training loss: 0.0072\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3800: Training loss: 0.0042\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4000: Training loss: 0.0045\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4200: Training loss: 0.0037\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4400: Training loss: 0.0071\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4600: Training loss: 0.0058\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4800: Training loss: 0.0048\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Test set: Average loss: 0.0006, Accuracy: 100.00\n",
      "Saving run: add_mod|num_epochs-5000|train_fraction-0.4|log_frequency-200|lr-0.001|batch_size-5017|weight_decay-1.5|use_transformer-True\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5017, Test size: 7527\n",
      "Using AlgorithmicDataset\n",
      "/home/rohit/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.8941\n",
      "Epoch 200: Training loss: 0.1673\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 400: Training loss: 0.0484\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 600: Training loss: 0.0303\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 800: Training loss: 0.0187\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1000: Training loss: 0.0112\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1200: Training loss: 0.0116\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1400: Training loss: 0.0114\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1600: Training loss: 0.0056\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 1800: Training loss: 0.0053\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2000: Training loss: 0.0080\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2200: Training loss: 0.0070\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2400: Training loss: 0.0046\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2600: Training loss: 0.0043\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 2800: Training loss: 0.0031\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3000: Training loss: 0.0079\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3200: Training loss: 0.0075\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3400: Training loss: 0.0076\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3600: Training loss: 0.0057\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 3800: Training loss: 0.0020\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4000: Training loss: 0.0044\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4200: Training loss: 0.0038\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4400: Training loss: 0.0042\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4600: Training loss: 0.0042\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Epoch 4800: Training loss: 0.0012\n",
      "Time taken for the last 200 epochs: 0.04 min\n",
      "Test set: Average loss: 9.2154, Accuracy: 4.93\n",
      "Saving run: add_mod|num_epochs-5000|train_fraction-0.4|log_frequency-200|lr-0.001|batch_size-5017|use_transformer-True\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 20: Training loss: 4.1792\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 40: Training loss: 2.2191\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 60: Training loss: 0.4341\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 80: Training loss: 0.0422\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 100: Training loss: 0.0134\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 120: Training loss: 0.0080\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 140: Training loss: 0.0058\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 160: Training loss: 0.0045\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 180: Training loss: 0.0036\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 200: Training loss: 0.0029\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 220: Training loss: 0.0024\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 240: Training loss: 0.0020\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 260: Training loss: 0.0017\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 280: Training loss: 0.0014\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 300: Training loss: 0.0012\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 320: Training loss: 0.0010\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 340: Training loss: 0.0009\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 360: Training loss: 0.0007\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 380: Training loss: 0.0006\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 400: Training loss: 0.0006\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 420: Training loss: 0.0005\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 440: Training loss: 0.0004\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 460: Training loss: 0.0004\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 480: Training loss: 0.0003\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Test set: Average loss: 25.4489, Accuracy: 0.05\n",
      "Saving run: add_mod|num_epochs-500|train_fraction-0.4|log_frequency-20|lr-0.005|batch_size-5107\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "cross_entropy\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 20: Training loss: 4.4877\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 40: Training loss: 4.1212\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 60: Training loss: 3.8797\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 80: Training loss: 3.6973\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 100: Training loss: 3.1735\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 120: Training loss: 2.3614\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 140: Training loss: 1.9346\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 160: Training loss: 1.4956\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 180: Training loss: 1.2340\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 200: Training loss: 1.1381\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 220: Training loss: 1.0778\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 240: Training loss: 1.0301\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 260: Training loss: 0.9938\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 280: Training loss: 0.9674\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 300: Training loss: 0.9463\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 320: Training loss: 0.9283\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 340: Training loss: 0.9128\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 360: Training loss: 0.8993\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 380: Training loss: 0.8877\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 400: Training loss: 0.8773\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 420: Training loss: 0.8680\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 440: Training loss: 0.8598\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 460: Training loss: 0.8523\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 480: Training loss: 0.8457\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Test set: Average loss: 0.9539, Accuracy: 98.73\n",
      "Saving run: add_mod|num_epochs-500|train_fraction-0.4|log_frequency-20|lr-0.005|batch_size-5107|orthogonal_gradients-True\n",
      "Using device: cuda:0\n",
      "Starting trining. Train dataset size: 5107, Test size: 7662\n",
      "Using AlgorithmicDataset\n",
      "stablemax\n",
      "/home/rohit/Code/Grokking-at-the-Edge-of-Numerical-Stability/logger.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.metrics_df = pd.concat([self.metrics_df, pd.DataFrame(rows)], ignore_index=True)\n",
      "Epoch 0: Training loss: 4.7273\n",
      "Epoch 20: Training loss: 4.7034\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 40: Training loss: 4.6327\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 60: Training loss: 4.4464\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 80: Training loss: 4.3729\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 100: Training loss: 4.3401\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 120: Training loss: 4.2339\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 140: Training loss: 3.9555\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 160: Training loss: 3.8558\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 180: Training loss: 3.7742\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 200: Training loss: 3.6977\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 220: Training loss: 3.3840\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 240: Training loss: 3.0830\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 260: Training loss: 2.7913\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 280: Training loss: 2.0218\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 300: Training loss: 0.7505\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 320: Training loss: 0.3976\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 340: Training loss: 0.3338\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 360: Training loss: 0.2947\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 380: Training loss: 0.2669\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 400: Training loss: 0.2461\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 420: Training loss: 0.2301\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 440: Training loss: 0.2170\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 460: Training loss: 0.2057\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Epoch 480: Training loss: 0.1964\n",
      "Time taken for the last 20 epochs: 0.00 min\n",
      "Test set: Average loss: 0.0026, Accuracy: 99.90\n",
      "Saving run: add_mod|num_epochs-500|train_fraction-0.4|optimizer-SGD|loss_function-stablemax|log_frequency-20|lr-10.0|batch_size-5107|softmax_precision-64|train_precision-64|orthogonal_gradients-True\n"
     ]
    }
   ],
   "source": [
    "!bash experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1635aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
